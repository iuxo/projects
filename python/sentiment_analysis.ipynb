{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Dave watched as the forest burned up on the hill,\n",
    "only a few miles from his house. The car had\n",
    "been hastily packed and Marta was inside trying to round\n",
    "up the last of the pets. \"Where could she be?\" he wondered\n",
    "as he continued to wait for Marta to appear with the pets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the word\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "token_list = [token for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[,\n",
       " Dave,\n",
       " watched,\n",
       " forest,\n",
       " burned,\n",
       " hill,\n",
       " ,,\n",
       " ,\n",
       " miles,\n",
       " house,\n",
       " .,\n",
       " car,\n",
       " ,\n",
       " hastily,\n",
       " packed,\n",
       " Marta,\n",
       " inside,\n",
       " trying,\n",
       " round,\n",
       " ,\n",
       " pets,\n",
       " .,\n",
       " \",\n",
       " ?,\n",
       " \",\n",
       " wondered,\n",
       " ,\n",
       " continued,\n",
       " wait,\n",
       " Marta,\n",
       " appear,\n",
       " pets,\n",
       " .,\n",
       " ]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words\n",
    "filtered_tokens = [token for token in doc if not token.is_stop]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Token: \\n, lemma: 962983613142996970_',\n",
       " 'Token: Dave, lemma: 15237984737769454380_',\n",
       " 'Token: watched, lemma: 2054481287215635300_',\n",
       " 'Token: forest, lemma: 12560106647199032635_',\n",
       " 'Token: burned, lemma: 12905682277821018784_',\n",
       " 'Token: hill, lemma: 1647358963876657122_',\n",
       " 'Token: ,, lemma: 2593208677638477497_',\n",
       " 'Token: \\n, lemma: 962983613142996970_',\n",
       " 'Token: miles, lemma: 15996833532744392865_',\n",
       " 'Token: house, lemma: 9471806766518506264_',\n",
       " 'Token: ., lemma: 12646065887601541794_',\n",
       " 'Token: car, lemma: 17545852598994811774_',\n",
       " 'Token: \\n, lemma: 962983613142996970_',\n",
       " 'Token: hastily, lemma: 16524687012062183671_',\n",
       " 'Token: packed, lemma: 11929990034961539164_',\n",
       " 'Token: Marta, lemma: 3686051643097225522_',\n",
       " 'Token: inside, lemma: 3410355712981309345_',\n",
       " 'Token: trying, lemma: 4812066089261065646_',\n",
       " 'Token: round, lemma: 10404471077220350636_',\n",
       " 'Token: \\n, lemma: 962983613142996970_',\n",
       " 'Token: pets, lemma: 8199115189604440881_',\n",
       " 'Token: ., lemma: 12646065887601541794_',\n",
       " 'Token: \", lemma: 15884554869126768810_',\n",
       " 'Token: ?, lemma: 8205403955989537350_',\n",
       " 'Token: \", lemma: 15884554869126768810_',\n",
       " 'Token: wondered, lemma: 17230765341337091640_',\n",
       " 'Token: \\n, lemma: 962983613142996970_',\n",
       " 'Token: continued, lemma: 7058970474994285413_',\n",
       " 'Token: wait, lemma: 6203382976377178934_',\n",
       " 'Token: Marta, lemma: 3686051643097225522_',\n",
       " 'Token: appear, lemma: 6170484565757444627_',\n",
       " 'Token: pets, lemma: 8199115189604440881_',\n",
       " 'Token: ., lemma: 12646065887601541794_',\n",
       " 'Token: \\n, lemma: 962983613142996970_']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize words\n",
    "lemmas = [\n",
    "        f\"Token: {token}, lemma: {token.lemma}_\"\n",
    "        for token in filtered_tokens\n",
    "]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0732638 , -1.589313  , -0.7485428 ,  0.8033879 ,  0.19977242,\n",
       "        0.00840408,  1.5419115 ,  0.78789234, -0.10507897, -0.08379465,\n",
       "        1.6370184 ,  0.99815553, -0.27276033, -0.9078418 , -1.2485981 ,\n",
       "       -0.5253064 ,  0.3660615 ,  0.32205266,  0.26947686, -0.6838585 ,\n",
       "       -1.3466268 ,  0.011222  , -0.24088496, -0.48466784, -0.33174878,\n",
       "       -0.05325326,  1.8773433 ,  0.56494987, -0.96057415,  0.78610957,\n",
       "       -0.44939822, -1.4648833 , -0.38066617,  1.0480769 , -0.8341217 ,\n",
       "       -0.2217494 , -0.85443366,  0.35594553, -0.1127467 ,  1.2787786 ,\n",
       "       -0.8223141 ,  0.18473059, -0.08983876,  0.6325263 , -1.1029459 ,\n",
       "        0.3719486 ,  0.11167981,  1.5298815 ,  0.73126984, -0.01238485,\n",
       "       -0.38741022,  0.24374121,  0.66934216, -0.51473886, -0.05107652,\n",
       "       -0.6836413 ,  1.2553529 , -0.4258146 ,  0.82571185, -0.40290013,\n",
       "       -1.0714419 ,  0.8215423 ,  0.10354415, -0.5627635 ,  0.34108153,\n",
       "       -0.46954852, -0.644461  , -0.4248718 , -0.74732137, -0.93381405,\n",
       "        0.54961896,  0.95218056,  1.8080062 ,  0.7719106 , -0.063205  ,\n",
       "       -1.1639179 , -0.3737961 , -2.050684  , -0.24220796, -0.6951563 ,\n",
       "       -0.6872523 ,  0.76149726, -0.8780628 ,  0.68690616,  1.4673213 ,\n",
       "       -0.3449757 ,  0.80576396, -0.3064615 , -0.64711714,  0.15497899,\n",
       "       -0.3926391 , -0.12381145,  2.0929089 ,  0.07248341,  1.388786  ,\n",
       "        1.6653655 ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize the text\n",
    "filtered_tokens[1].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THE_ONE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba21b04416bd6bb7d943d6e1e30db1b014094e2c4596ae230c1b8e6f1c8e2c12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
